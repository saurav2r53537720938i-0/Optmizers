{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70b43b4-ba5f-4324-b6c0-e7c2062e6aac",
   "metadata": {},
   "source": [
    "optimization algorthims play a crucial role in traning a special networks between the all department actuatal ANN large parmetrs making manual adjusment impractical optmization akgorthims automate the process of finding the optimal paramters value allowing ANN to learn complex patterns and relationship in data efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd89d945-f109-45ae-94a5-a47f01649e4a",
   "metadata": {},
   "source": [
    "Gradient descent is an optmization algorthim used to minimize a loss functionby iterativelyadjusting in the direction of the steepest descent of the gradient descent of the gradient here how it works:\n",
    "1. Intialize\n",
    "2. Compute gradient\n",
    "3. Update paramter\n",
    "4. Repeat\n",
    "variant of gradient descent differ in how they update parmters and handle the learning rate\n",
    "1. Batch gradient descent\n",
    "2. Stochastic gradient Descent\n",
    "3. Mini batch gradeint decsent\n",
    "4. Momentum\n",
    "5. Adaptive learning rate methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4184f-500c-44e7-9057-b399511f4288",
   "metadata": {},
   "source": [
    "Tradtional gradient descent optmization method:\n",
    "1. Convergence speed:Tradtional gradient descent can be slow to converge especially on complex or ill-condtional surface.\n",
    "2. Local minima:it can get stuck in local minima falling to find the global minium\n",
    "3. Senstvity to learn rate:choosing an appropriate learningrate can be challenging as a toolarge or too small value can hinder convergence.\n",
    "Modern optimizers adress these challenges\n",
    "1. Accelerated Convergence:algorthims like momentum,adagard RMSprop and adam incorprate momentum or adaptive learning\n",
    "2. Escape from local minima\n",
    "3. adapative learning rates\n",
    "4. Second order methods\n",
    "5. regularizzation\n",
    "Each modern optmizers has its strength and weakness and the choice depends on factor like the problems charterstics computational resources and emprical perfomance on similar tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a14720-136e-40ea-b591-c15656d21654",
   "metadata": {},
   "source": [
    "Istorto refers to a gradient descent variant that levrage curvature information to adjust the step size during optimization in tradtional gradient descent the step size remains consant or is adjusted based sloely on the gradeint magtiude however istorto takes into account the curvature more adapative step sizes\n",
    "Advantage of isforto compared to trdational gradient descent\n",
    "1. Faster converges\n",
    "2. Improved robustness\n",
    "3. better handling ofill condtioned problebs\n",
    "Limitations of isforto:\n",
    "1. Computational cost\n",
    "2. Potential Overshooting\n",
    "Scenrios where istorto is most suitable:\n",
    "1. Non-convex optimization\n",
    "2. Ill-condtioned problembs\n",
    "3. Large scale optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a29c2b-f292-4b90-ba7a-301ebada0c0f",
   "metadata": {},
   "source": [
    "The adam optimizers is a popular optimization algorthim used in traning deep neural networks it combines the concept of momentum and adpative learning rates to effficently update model parmters during the traning process\n",
    "1. Momentum\n",
    "2. Adaptive learning Rates\n",
    "Benfits of adamoptimizers\n",
    "1. Efficent Convergence\n",
    "2. Robustness "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc466c91-2a4c-46aa-aba4-43c48d282a4d",
   "metadata": {},
   "source": [
    "RMSProp short for root mean square propgation and ADAm which stands for adapative moment estimation are both optimization algorthims commonly used in traning neural networks.Rmsprop adresses the challenge of adaptive learning by adjusting the learning by adjusting learning rate of each paramter based on the magntiude of recent gradient it helps in speeding up convergence overcoming the problebs of vanuhing or exploding gradeint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
